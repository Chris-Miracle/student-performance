{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1EWoZyZuoG6FVe8YkDNccO0GIXkImpAtU",
      "authorship_tag": "ABX9TyOam8ViaNT9wq5PLAlRTGns"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Loading Libraries"
      ],
      "metadata": {
        "id": "XJzgrND3OIp4"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "P534PxKFN0p1"
      },
      "outputs": [],
      "source": [
        "# Import necessary libraries\n",
        "import os\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.neural_network import MLPRegressor\n",
        "from sklearn.ensemble import GradientBoostingRegressor\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load all necessary libraries for prediction objective."
      ],
      "metadata": {
        "id": "en2To7XyOeNi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Preparation"
      ],
      "metadata": {
        "id": "4vZwzrNvOoAQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Connect google drive directory\n",
        "os.chdir('/content/drive/MyDrive/Colab/Datasets/')\n",
        "df = pd.read_csv('Student_Performance.csv')\n",
        "\n",
        "# Check for null values\n",
        "df.isnull().sum()\n",
        "\n",
        "# Drop nan values if any\n",
        "df.dropna(inplace=True)\n",
        "\n",
        "# Remove duplicates\n",
        "df.drop_duplicates(inplace=True)\n",
        "\n",
        "# Run One-hot encoding for Extracurricular Activites\n",
        "df_encoded = pd.get_dummies(df, columns=['Extracurricular Activities'], drop_first=True)\n",
        "\n",
        "# Split data training 80%, test 20%\n",
        "X = df_encoded.drop('Performance Index', axis=1)\n",
        "y = df_encoded['Performance Index']\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Check sizes\n",
        "print(f'Training set: {X_train.shape}')\n",
        "print(f'Test set: {X_test.shape}')"
      ],
      "metadata": {
        "id": "KbFsjwrKOnPd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c753626f-e9cf-4557-8504-64082aec4721"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training set: (7898, 5)\n",
            "Test set: (1975, 5)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load dataset, check for null values, drop nan values, remove duplicates. Convert 'Extracurricular Activities' to categorical data. Split dataset to features and target variable finally split dataset 80% training, 20% test data"
      ],
      "metadata": {
        "id": "CmRgDty3UR5O"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Modeling"
      ],
      "metadata": {
        "id": "1VwBCvQKU8Ct"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Neural Network Model"
      ],
      "metadata": {
        "id": "gHbhf1f7bY0b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform Neural Network regression on train data\n",
        "model = MLPRegressor(hidden_layer_sizes=(10,), max_iter=1000)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "predictions_neural = model.predict(X_test)"
      ],
      "metadata": {
        "id": "z4F3NHH6Vnl5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The MLPRegressor, takes in key parameters that define its structure and training process.\n",
        "\n",
        "hidden_layer_sizes=(10,): Specifies the number of neurons in the hidden layer. In this case, there is one hidden layer with 10 neurons.\n",
        "\n",
        "max_iter=1000: Determines the maximum number of training iterations before the model stops updating its weights.\n",
        "\n",
        "Training Process\n",
        "\n",
        "The input features (X) are fed into the network.\n",
        "\n",
        "Each neuron multiplies the inputs by weights, adds a bias\n",
        "\n",
        "The processed values pass through 10 hidden layer neurons, where computations continue.\n",
        "\n",
        "The output layer generates a predicted value ŷ.\n",
        "\n",
        "The model compares ŷ with the actual target (y) and calculates an error (loss).\n",
        "\n",
        "Using backpropagation, the model adjusts the weights to minimize the error.\n",
        "\n",
        "Steps 1-6 repeat for up to 1,000 iterations or until the model converges (finds the best weights)."
      ],
      "metadata": {
        "id": "bAL3qMppbjP8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Gradient Boosting"
      ],
      "metadata": {
        "id": "pqjcaHBpdynS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform Gradient Boosting regression on the train data\n",
        "gb_regressor = GradientBoostingRegressor(n_estimators=100, learning_rate=0.1, max_depth=3, random_state=42)\n",
        "gb_regressor.fit(X_train, y_train)\n",
        "\n",
        "predictions_gradient = gb_regressor.predict(X_test)"
      ],
      "metadata": {
        "id": "LSx8qEiGdTHh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Key parameters:\n",
        "\n",
        "n_estimators=100: Specifies the number of decision trees in the ensemble. In this case, the model builds 100 trees.\n",
        "\n",
        "learning_rate=0.1: Controls how much each tree contributes to the final prediction. A lower value makes training more gradual and prevents overfitting.\n",
        "\n",
        "max_depth=3: Defines the depth of each decision tree, limiting how complex each tree can be. Here, each tree can have up to 3 levels.\n",
        "\n",
        "random_state=42: Ensures the results are reproducible by controlling the randomness in training.\n",
        "\n",
        "Training Process\n",
        "\n",
        "The model starts with a simple prediction (e.g., the average target value).\n",
        "\n",
        "It builds the first decision tree to predict the residual errors (differences between actual and predicted values).\n",
        "\n",
        "The next tree learns from the previous errors and makes corrections.\n",
        "\n",
        "This process repeats 100 times, with each new tree improving the overall prediction.\n",
        "\n",
        "The final prediction is the sum of all the trees’ outputs, scaled by the learning rate."
      ],
      "metadata": {
        "id": "CZbed7uNfEfP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "39skjj67fRnX"
      }
    }
  ]
}